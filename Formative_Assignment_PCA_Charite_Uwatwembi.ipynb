{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charite-uwatwembi/MachineLearning-PCA/blob/main/Formative_Assignment_PCA_Charite_Uwatwembi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "import numpy as np\n",
        "import numpy.linalg as eig\n",
        "#TO DO\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #TO DO:  Insert code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "avg = np.mean(data, axis=0)\n",
        "std_dev = np.std(data, axis=0)\n",
        "standardized_data = (data - avg) / std_dev\n",
        "print(standardized_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a005bea9-bda7-4f49-e15c-9035c5602ca3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "bce379ba-593d-46be-e11d-d3917873f20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        " #TO DO: insert code here\n",
        "\n",
        "\n",
        "# Compute the covariance matrix manually\n",
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "print(cov_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " #TO DO: Insert code here for eigenvalues, eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"\\nEigenvalues:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nEigenvectors:\")\n",
        "print(eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8824acb5-6af9-4fd5-fd4d-df7b0eacc41b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Eigenvalues:\n",
            "[3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "\n",
            "Eigenvectors:\n",
            "[[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the order of importance (highest to lowest)\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print('\\nThe order of importance is:\\n{}'.format(order_of_importance))\n",
        "\n",
        "# Utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "print('\\n\\nSorted eigenvalues:\\n{}'.format(sorted_eigenvalues))\n",
        "\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\nThe sorted eigenvector matrix is:\\n{}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "1af93aeb-474e-401a-ab2f-fca6c5e13276"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The order of importance is:\n",
            "[0 1 4 2 3]\n",
            "\n",
            "\n",
            "Sorted eigenvalues:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            "The sorted eigenvector matrix is:\n",
            "[[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "[Ordering eigenvalues and eigenvectors enables us to prioritize the most significant directions of data variation, with the importance of each eigenvalue reflecting the variance it explains. Sorting them from highest to lowest allows us to identify key components, where the largest eigenvalues correspond to the most important directions or principal components capturing the most variance. This ordering facilitates dimensionality reduction in techniques like Principal Component Analysis (PCA), where we project the data onto the top principal components, simplifying it while retaining maximal information. Moreover, ordered eigenvalues and their corresponding eigenvectors aid in interpreting the structure and patterns within the data, streamlining data interpretation processes.]\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "[No, In eigenvalue-eigenvector analysis, we prioritize the highest eigenvalues over the lowest ones because they represent the directions in the data space where the most variance occurs. These directions capture the most significant patterns or structures in the data, making them crucial for tasks like Principal Component Analysis (PCA) where dimensionality reduction aims to retain the most informative features. By focusing on the highest eigenvalues, we extract the most important components of the data for analysis, interpretation, and modeling purposes.]\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "sorted_eigenvalues = [3.80985761e+00, 1.73655615e+00, 4.04085720e-01, 4.94531029e-02, 4.74189469e-05]\n",
        "\n",
        "total_variance = sum(sorted_eigenvalues)\n",
        "\n",
        "explained_variance = [(eigval / total_variance) * 100 for eigval in sorted_eigenvalues]\n",
        "\n",
        "explained_variance = [\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "13a86491-ce78-40e4-ebfe-5c134abd04e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2 # select the number of principal components\n",
        "\n",
        "\n",
        "reduced_data = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data, reduced_data)"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "a20359b7-ecee-43cd-f2cc-96be37a4a8d8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.3577116  -0.75728867]\n",
            " [-2.27171739 -1.81970663]\n",
            " [ 1.21259114 -0.50390931]\n",
            " [-1.41935914  1.9229856 ]\n",
            " [ 1.61562536  0.87541857]\n",
            " [-1.49485157  0.28250044]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of reduced data:\", reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtOMdS2_4LIp",
        "outputId": "01f271f4-ee2d-437b-a5a4-2fe407a9aaa3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of reduced data: (6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "[insert answer here]\n",
        "\n",
        "\n",
        "**Positive Effects: **\n",
        "\n",
        "1. **Dimensionality Reduction:** PCA effectively reduces the dimensionality of the data by transforming it into a new set of variables called principal components. This reduction simplifies the dataset, making it easier to visualize, analyze, and interpret, while still retaining most of the original information. It helps in dealing with high-dimensional data, improving computational efficiency, and facilitating data exploration.\n",
        "\n",
        "2. **Feature Extraction:** PCA identifies the most important features or patterns in the data by capturing the directions of maximum variance. These principal components are orthogonal to each other, meaning they represent independent sources of variation. By focusing on these components, PCA enables feature extraction, allowing for the removal of noise or irrelevant features and emphasizing the most significant ones. This feature extraction process aids in improving the performance of machine learning models by focusing on the most informative aspects of the data.\n",
        "\n",
        "**Negative Effects:**\n",
        "\n",
        "1. **Loss of Interpretability:** While PCA simplifies the data by reducing its dimensionality, it may also lead to a loss of interpretability. The transformed principal components are linear combinations of the original features, making it challenging to interpret them directly in terms of the original variables. This loss of interpretability can hinder the understanding of underlying patterns or relationships in the data, especially when trying to communicate findings to stakeholders or domain experts.\n",
        "\n",
        "2. **Assumption of Linearity:** PCA assumes that the underlying relationships in the data are linear. However, if the data exhibits nonlinear relationships, PCA may not effectively capture all the variations, leading to suboptimal results. In such cases, nonlinear dimensionality reduction techniques like t-distributed Stochastic Neighbor Embedding (t-SNE) or Isomap may be more appropriate. Therefore, it's essential to assess whether the linear assumption holds true for the dataset before applying PCA to avoid misinterpretations or inaccuracies in the results."
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}